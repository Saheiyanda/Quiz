{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-9ec6fb82a481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Importing useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, \\\n",
    "                    LeaveOneOut, KFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import sklearn.utils\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('./Data_for_UCI_named.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'stab' column as instructed\n",
    "df = df.drop('stab', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop('stabf') # Pops out the stabf column as the label\n",
    "X = df # Uses the remaining columns as the features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into 80:20 training and testing test with a random_state of 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # Initializes a StandardScaler object\n",
    "scaled_X_train = scaler.fit_transform(X_train) # Fits and transform the training set\n",
    "scaled_X_test = scaler.transform(X_test) # Transforms the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a function that returns the metric score of a test set. The metric can be any of  accuracy_score, precision_score, recall_score, f1_score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a dictionary of the metrices we will be using\n",
    "metrics = {'accuracy_score': accuracy_score, 'precision_score': precision_score, 'recall_score': recall_score, \n",
    "               'f1_score': f1_score, 'confusion_matrix': confusion_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function\n",
    "def get_metric_score(metric, ytrue, ypred, neg_pos_label):\n",
    "    ''' This function returns the specified metric score. It only works with classifier metrics.\n",
    "        \n",
    "        Args:   metric (string): the evaluating metric, can be any of accuracy_score, precision_score, recall_score, f1_score, \n",
    "                                 or confusion matrix.\n",
    "                ytrue (array): the true labels\n",
    "                ypred (array): the predicted labels\n",
    "                neg_pos_label (list): a list of the classes you want as the negative and positive label in order \n",
    "                                      of [negative_label, positive_label]\n",
    "                \n",
    "        Return: returns the metric score\n",
    "    '''\n",
    "    \n",
    "    if metric == 'accuracy_score':\n",
    "        return accuracy_score(ytrue, ypred)\n",
    "    \n",
    "    elif metric == 'confusion_matrix':\n",
    "        return confusion_matrix(ytrue, ypred, neg_pos_label)\n",
    "    \n",
    "    else:\n",
    "        return metrics[metric](ytrue, ypred, pos_label=neg_pos_label[1]) # this is done because precision, recall and f1_score\n",
    "                                                                         # takes the same arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a that function fits a classifier on a training set and prints out the accuracy_score, precision_score, recall_score,    f1_score and confusion matrix of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function\n",
    "def fit_and_score(classifier, xtrain, ytrain, xtest, ytest, neg_pos_label):\n",
    "    ''' This function fits a classifier on a training set and prints out the accuracy_score, precision_score, recall_score, \n",
    "    f1_score and confusion matrix of the testing set.\n",
    "    \n",
    "    Args: classifier (classifier object): the classifier you want to use\n",
    "          xtrain (ndarray): the training features\n",
    "          ytrain (array): the training labels\n",
    "          xtest (ndarray): the testing features\n",
    "          ytest (array): the testing labels\n",
    "          neg_pos_label (list): a list of the classes you want as the negative and positive label in order \n",
    "                                      of [negative_label, positive_label]\n",
    "    '''\n",
    "    classifier.fit(xtrain, ytrain) # fits the classifier\n",
    "    ypred = classifier.predict(xtest) # predicts\n",
    "    \n",
    "    # for each metric in metrics (dictionary earlier defined), print out the metric score.\n",
    "    for metric in metrics:\n",
    "        \n",
    "        # this 'if' block is to ensure that the confusion matrix is properly printed out to improve redability\n",
    "        if metric == 'confusion_matrix':\n",
    "            print()\n",
    "            print('confusion_matrix is:')\n",
    "            print(get_metric_score(metric, ytest, ypred, neg_pos_label))\n",
    "            \n",
    "        else:\n",
    "            print('{} is {}'.format(metric, get_metric_score(metric, ytest, ypred, neg_pos_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['unstable', 'stable'] # this is to be used as the neg_pos_label needed in fit_and_score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate our model on different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing on RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=1)\n",
    "fit_and_score(random_forest, scaled_X_train, y_train, scaled_X_test, y_test, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees = ExtraTreesClassifier(random_state=1)\n",
    "fit_and_score(extra_trees, scaled_X_train, y_train, scaled_X_test, y_test, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(random_state=1)\n",
    "fit_and_score(xgb, scaled_X_train, y_train, scaled_X_test, y_test, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lightgbm.LGBMClassifier(random_state=1)\n",
    "fit_and_score(lgbm, scaled_X_train, y_train, scaled_X_test, y_test, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing search space of hyperparameters\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "# making a dictionary of the grid\n",
    "hyperparameter_grid = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf,\n",
    "                       'min_samples_split': min_samples_split, 'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees2 = ExtraTreesClassifier(random_state=1) # initializes an ExtraTreesClassifier\n",
    "\n",
    "# initializing a RandomizedSearchCV\n",
    "tuned_extra_trees = RandomizedSearchCV(extra_trees2, hyperparameter_grid, random_state=1, verbose=1, n_jobs=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score(tuned_extra_trees, scaled_X_train, y_train, scaled_X_test, y_test, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest2 = RandomForestClassifier(random_state=1) # initializes a random forest classifier object\n",
    "random_forest2.fit(scaled_X_train, y_train) # fits the model\n",
    "ypred_random_forest = random_forest2.predict(scaled_X_test) # predicts on the testing set\n",
    "accuracy_score(y_test, ypred_random_forest) # ouputs the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = xgboost.XGBClassifier(random_state=1) # initializes a xgboost classifier object\n",
    "xgb2.fit(scaled_X_train, y_train) # fits the model\n",
    "ypred_xgb = xgb2.predict(scaled_X_test) # predicts on the testing set\n",
    "accuracy_score(y_test, ypred_xgb) # ouputs the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_gbm2 = lightgbm.LGBMClassifier(random_state=1) # initializes a light gbm classifier object\n",
    "light_gbm2.fit(scaled_X_train, y_train) # fits the model\n",
    "ypred_light_gbm2 = light_gbm2.predict(scaled_X_test) # predicts on the testing set\n",
    "accuracy_score(y_test, ypred_light_gbm2) # ouputs the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing search space of hyperparameters\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', None] \n",
    "\n",
    "# making a dictionary of the grid\n",
    "hyperparameter_grid = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf, \n",
    "                       'min_samples_split': min_samples_split, 'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees2 = ExtraTreesClassifier(random_state=1) # initializes an ExtraTreesClassifier object\n",
    "\n",
    "# initializing a RandomizedSearchCV object\n",
    "tuned_extra_trees2 = RandomizedSearchCV(extra_trees2, hyperparameter_grid, cv=5, n_iter=10, scoring = 'accuracy', n_jobs = -1, \n",
    "                                        verbose = 1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the tuned_extra_trees2 model\n",
    "tuned_extra_trees2.fit(scaled_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best hyperparameters\n",
    "tuned_extra_trees2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train an ordinary extra trees classifier\n",
    "extra_trees3 = ExtraTreesClassifier(random_state=1) # initializes an extra trees classifier object\n",
    "extra_trees3.fit(scaled_X_train, y_train) # fits the extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_ordinary = extra_trees3.predict(scaled_X_test) # predicts on the testing set\n",
    "\n",
    "print('accuracy of ordinary extra trees is {}'.format(accuracy_score(y_test, ypred_ordinary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's find the accuracy of the tuned extra trees\n",
    "ypred_tuned = tuned_extra_trees2.best_estimator_.predict(scaled_X_test) # predicts with the best estimator of the tuned extra \n",
    "                                                                        # trees\n",
    "print('accuracy of tuned extra trees is {}'.format(accuracy_score(y_test, ypred_tuned)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = tuned_extra_trees2.best_estimator_.feature_importances_ # finds the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_feature = features_importance.max() # finds the most important feature\n",
    "least_important_feature = features_importance.min() # finds the least important feature\n",
    "\n",
    "cols = X.columns # assigns the features in the data to a variable 'cols'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('most important feature is {}'.format(cols[features_importance == most_important_feature][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('least important feature is {}'.format(cols[features_importance == least_important_feature][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
